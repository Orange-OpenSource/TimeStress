{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software Name : TimeStress\n",
    "# SPDX-FileCopyrightText: Copyright (c) Orange SA\n",
    "# SPDX-License-Identifier: MIT\n",
    "\n",
    "# This software is distributed under the MIT License,\n",
    "# see the \"LICENSE.txt\" file for more details or https://spdx.org/licenses/MIT.html\n",
    "\n",
    "# Authors: see CONTRIBUTORS.md\n",
    "# Software description: Evaluating the Consistency of the Temporal Representation of Facts in Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Goal of notebook**: Analysis of the language models' predictions on TimeStress\n",
    "\n",
    "This notebook is used to generate the figures and tables present in our [paper](https://arxiv.org/abs/2502.01220)\n",
    "\n",
    "**How to use this notebook?**\n",
    "\n",
    "1. Create a folder `outputs` in the same folder as this notebook.\n",
    "1. Put all the language models' predictions inside `outputs` (the ones generated using the `run.py` script).\n",
    "1. Set the `EXPERIMENT` constant accordingly (see below for details), and run all the cells in this notebook. \n",
    "1. The plots will be generated in a new folder called `plots`.\n",
    "\n",
    "Choose `EXPERIMENT` value from the follozing list\n",
    "   - `classic`: Most tables and figures can be generated with this setting\n",
    "   - `explain_granularity_generalization` and `explain_date`: These 2 settings are used to generate the *Explanation Prompts* results in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'classic' # choose from ['classic', 'explain_granularity_generalization', 'explain_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from models_metadata import is_instruct, get_model_num_params, get_instruct_version_of_model, get_classical_version_of_model, get_model_family, extract_metadata_from_filename\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ke_utils.glob_core import TimeUnit\n",
    "from wikidata_tools.core import Date, enable_level_heterogeneous_comparison\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "KEPT_MODELS = ['google_gemma-2-27b-it',\n",
    "                'google_gemma-2-9b-it',\n",
    " 'meta-llama_Llama-3.1-70B-Instruct',\n",
    " 'mistralai_Mistral-7B-Instruct-v0.3',\n",
    " \n",
    "]\n",
    "def load_data() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    dfs = []\n",
    "    models_data = []\n",
    "    models_cols = ['Model', \"Instruct\", \"NumParams\"]\n",
    "    for p in Path('outputs/temporality_paper').glob('*__experiment=%s.pkl' % EXPERIMENT):\n",
    "        print('Loading %s' % p.name)\n",
    "        model = extract_metadata_from_filename(p.name)['model']\n",
    "        inst = is_instruct(model)\n",
    "        nparams = get_model_num_params(model)\n",
    "        models_data.append((model, inst, nparams))\n",
    "        df = pd.read_pickle(p)\n",
    "        df['Model'] = model\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs), pd.DataFrame(data=models_data, columns=models_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs, models_data = load_data()\n",
    "\n",
    "# Remove future dates\n",
    "REFERENCE_TIME = Date(np.datetime64('2021-01-04'))\n",
    "with enable_level_heterogeneous_comparison():\n",
    "    logprobs = logprobs[logprobs['Time'].apply(lambda x : Date(x) < REFERENCE_TIME if isinstance(x,np.datetime64) else True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that for each Fact, there is the same number of tests (times) per granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs[logprobs['IsCorrect'] == 'Transitional']['Alpha'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_dedup = logprobs.copy()\n",
    "\n",
    "# Associate all Transitional dates to an alpha=+-0.5 to have a better visualization in alpha vs. logprob plots\n",
    "logprobs_dedup.loc[logprobs_dedup['IsCorrect'] == 'Transitional', 'Alpha'] = logprobs_dedup.loc[logprobs_dedup['IsCorrect'] == 'Transitional', 'Alpha'].apply(lambda value : min([-0.5,0.5], key=lambda x:abs(x-value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one test per relation\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# rel = logprobs['Fact'].apply(lambda x : x.relation.id)\n",
    "x = logprobs_dedup.groupby(['IsCorrect'])[['Fact', 'Time', 'IsCorrect', 'Statement']].sample(2)\n",
    "print(x.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "unique_dates_per_fact = logprobs.groupby(['Fact', 'Granularity'], sort=False)['Time'].unique().apply(len).reset_index()\n",
    "sns.histplot(unique_dates_per_fact, x='Time', hue='Granularity')\n",
    "plt.title('Before deduplication')\n",
    "\n",
    "plt.figure()\n",
    "unique_dates_per_fact = logprobs_dedup.groupby(['Fact', 'Granularity'], sort=False)['Time'].unique().apply(len).reset_index()\n",
    "sns.histplot(unique_dates_per_fact, x='Time', hue='Granularity')\n",
    "plt.title('After deduplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add order score\n",
    "def f(df : pd.DataFrame):\n",
    "    x = df['CondLogProb'].values\n",
    "    wins = (x[:, None] > x[None, :]).sum(-1) / (len(x)-1)\n",
    "    return pd.Series(wins, index=df.index)\n",
    "logprobs_dedup['FactStr'] = logprobs_dedup['Fact'].apply(str)\n",
    "logprobs_dedup = logprobs_dedup.sort_values(by=['Model', 'FactStr', 'Granularity', 'Alpha']).reset_index(drop=True)\n",
    "logprobs_dedup['TimeWin'] = logprobs_dedup[['Model', 'FactStr', 'Granularity', 'CondLogProb']].groupby(['Model', 'FactStr', 'Granularity'], sort=False).apply(f).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(df : pd.DataFrame):\n",
    "    correct_mask = df['IsCorrect'] == 'Correct'\n",
    "    incorrect_mask = df['IsCorrect'] == 'Incorrect'\n",
    "    matches = df.loc[correct_mask, 'CondLogProb'].values[:, None] > df.loc[incorrect_mask, 'CondLogProb'].values[None, :]\n",
    "    wrong1, wrong2 = np.where(~matches)\n",
    "    alpha1, alpha2 = df.loc[correct_mask, 'Alpha'].iloc[wrong1].values, df.loc[incorrect_mask, 'Alpha'].iloc[wrong2].values\n",
    "    soft_score = matches.mean()\n",
    "    hard_score = soft_score == 1\n",
    "    if not df['CondLogProbInstruct'].isna().all():\n",
    "        matches_inst = df.loc[correct_mask, 'CondLogProbInstruct'].values[:, None] > df.loc[incorrect_mask, 'CondLogProbInstruct'].values[None, :]\n",
    "        soft_score_inst = matches_inst.mean()\n",
    "        hard_score_inst = soft_score_inst == 1\n",
    "        matches_inst = df.loc[correct_mask, 'CondLogProbInstruct'].values[:, None] > df.loc[incorrect_mask, 'CondLogProbInstruct'].values[None, :]\n",
    "        wrong1, wrong2 = np.where(~matches_inst)\n",
    "        alpha1_inst, alpha2_inst = df.loc[correct_mask, 'Alpha'].iloc[wrong1].values, df.loc[incorrect_mask, 'Alpha'].iloc[wrong2].values\n",
    "    else:\n",
    "        soft_score_inst = float('nan')\n",
    "        hard_score_inst = float('nan')\n",
    "        alpha1_inst, alpha2_inst = None, None\n",
    "    \n",
    "    model = df['Model'].iloc[0]\n",
    "    fact = df['Fact'].iloc[0]\n",
    "    granularity = df['Granularity'].iloc[0]\n",
    "    return pd.DataFrame(data = {\n",
    "        'Model' : [model]*2,\n",
    "        'Granularity' : [granularity.name.lower().capitalize()]*2,\n",
    "        'Fact' : [fact]*2,\n",
    "        'Instruct' : [False, True],\n",
    "        'AccSoft' : [soft_score, soft_score_inst],\n",
    "        'AccHard' : [hard_score, hard_score_inst],\n",
    "        'WrongAlphaIncorrect' : [alpha2, alpha2_inst],\n",
    "        'WrongAlphaCorrect' : [alpha1, alpha1_inst]\n",
    "    })\n",
    "score_per_fact = logprobs_dedup.groupby(['Model', 'Fact', 'Granularity'], sort=False).apply(f).reset_index(drop=True)\n",
    "\n",
    "GLOBALGR = \"Global\"\n",
    "all_hard = score_per_fact.groupby(['Model', 'Fact', 'Instruct'], sort=False)['AccHard'].apply(lambda x : float('nan') if x.isna().all() else (x == 1).all())\n",
    "all_soft = score_per_fact.groupby(['Model', 'Fact', 'Instruct'], sort=False)['AccSoft'].mean()\n",
    "all_ = pd.concat([all_hard, all_soft], axis=1).reset_index()\n",
    "all_['Granularity'] = GLOBALGR\n",
    "\n",
    "score_per_fact = pd.concat([score_per_fact, all_])\n",
    "score_per_fact['AccHard'] = score_per_fact['AccHard'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the performance of a random baseline for zwin rate\n",
    "logprobs_random = logprobs_dedup[logprobs_dedup['Model'] == 'apple_OpenELM-3B'].copy()\n",
    "logprobs_random['CondLogProb'] = np.random.random(len(logprobs_random))\n",
    "logprobs_random['CondLogProbInstruct'] = np.random.random(len(logprobs_random))\n",
    "\n",
    "score_per_fact_random = logprobs_random.groupby(['Model', 'Fact', 'Granularity'], sort=False).apply(f).reset_index(drop=True)\n",
    "print('Random baseline performance:')\n",
    "score_per_fact_random[['AccSoft', 'AccHard']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_per_fact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_per_fact['Model'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many popular facts LLMs know? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-instruct queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sns.set(font_scale=1.1)\n",
    "\n",
    "data = score_per_fact[~score_per_fact['Instruct']]\n",
    "data['Model'] = data['Model'].apply(lambda x : x.split('_', 1)[-1])\n",
    "order = data.groupby('Model')['AccSoft'].mean().sort_values().index.tolist()\n",
    "plt.figure(figsize=(5,7))\n",
    "sns.barplot(data=data, y='Model', x='AccSoft', hue='Granularity', order=order)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--');\n",
    "plt.xlim((0.4,1))\n",
    "plt.xlabel('Average $\\mathcal{W}$')\n",
    "plt.ylabel('')\n",
    "plt.text(0.53, 10, 'Random', color='black', fontsize=15,  ha='center', va='bottom', rotation=90);\n",
    "plt.tight_layout()\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "plt.savefig('plots/lenient_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.1)\n",
    "\n",
    "data = score_per_fact[~score_per_fact['Instruct']]\n",
    "data['Model'] = data['Model'].apply(lambda x : x.split('_', 1)[-1])\n",
    "# data['AccHard'] = data['AccHard'].astype(float)\n",
    "order = data[data['Granularity'] == GLOBALGR].groupby('Model')['AccHard'].mean().sort_values().index.tolist()\n",
    "plt.figure(figsize=(5,7))\n",
    "sns.barplot(data=data, y='Model', x='AccHard', hue='Granularity', order=order)\n",
    "plt.xlim((0,0.15));\n",
    "plt.xlabel('Average $\\mathcal{R}$')\n",
    "plt.ylabel('');\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct models vs. Non-instruct models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = score_per_fact[~score_per_fact['Instruct'] & (score_per_fact['Granularity'] == GLOBALGR)]\n",
    "classical_models = [x for x in data['Model'].apply(get_classical_version_of_model).unique() if x is not None]\n",
    "instruct_models = [x for x in data['Model'].apply(get_instruct_version_of_model).unique() if x is not None]    \n",
    "data_classical = data[np.isin(data['Model'], classical_models)]\n",
    "data_instruct : pd.DataFrame = data[np.isin(data['Model'], instruct_models)]\n",
    "data_instruct['ModelClassical'] = data_instruct['Model'].apply(get_classical_version_of_model)\n",
    "assert data_instruct['ModelClassical'].shape[0] == data_classical.shape[0]\n",
    "versus_data = data_instruct.merge(data_classical, left_on=['ModelClassical', 'Fact', 'Granularity'], right_on=['Model', 'Fact', 'Granularity'], suffixes=['', 'Classical'])\n",
    "column_numbers = [x for x in range(versus_data.shape[1])]\n",
    "column_numbers.remove(versus_data.columns.tolist().index('ModelClassical')) # Remove Duplicate column\n",
    "versus_data = versus_data.iloc[:, column_numbers]\n",
    "\n",
    "versus_data['Versus $\\mathcal{W}$'] = versus_data[['AccSoft', 'AccSoftClassical']].apply(lambda x : 'Instruct-tuned Wins' if x['AccSoft'] > x[\"AccSoftClassical\"] else \"Pretrained Wins\" if x['AccSoft'] < x[\"AccSoftClassical\"] else 'Tie', axis=1)\n",
    "versus_data['Versus $\\mathcal{R}$'] = versus_data[['AccHard', 'AccHardClassical']].apply(lambda x : 'Instruct-tuned Wins' if x['AccHard'] > x[\"AccHardClassical\"] else \"Pretrained Wins\" if x['AccHard'] < x[\"AccHardClassical\"] else 'Tie', axis=1)\n",
    "versus_data['ModelClassical'] = versus_data['ModelClassical'].apply(lambda x : x.split('_', 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# display(versus_data.groupby(['ModelClassical', 'Versus-Lenient'])['Model'].count().to_frame())\n",
    "plt.figure(figsize=(6,4))\n",
    "n_unique_models = len(versus_data['ModelClassical'].unique())\n",
    "sns.histplot(data=versus_data, y='ModelClassical', hue='Versus $\\mathcal{W}$', weights=(1/versus_data.shape[0])*n_unique_models, multiple='stack', hue_order=['Pretrained Wins', 'Tie', 'Instruct-tuned Wins'])\n",
    "plt.axvline(x=0.5, color='black', linestyle='--');\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Proportion')\n",
    "plt.savefig('plots/inst_vs_noninst_lenient_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instruct queries\n",
    "\n",
    "Does Intruct-format helps predicting the right answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"WrongAlphaIncorrect\", \"WrongAlphaCorrect\"]).groupby(['Model', 'Granularity', 'Instruct'])[['AccSoft', 'AccHard']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "\n",
    "instruct_models = models_data.loc[models_data['Instruct'], 'Model']\n",
    "data = score_per_fact[np.isin(score_per_fact['Model'], instruct_models)]\n",
    "avg = data.groupby(['Instruct', 'Fact', 'Granularity'], sort=False)[['AccHard', 'AccSoft']].mean().reset_index()\n",
    "avg['Model'] = 'Average'\n",
    "data = pd.concat([data, avg], ignore_index=True)\n",
    "data['Model'] = data['Model'].apply(lambda x : x.split('_', 1)[-1])\n",
    "data['Average $\\mathcal{R}$'] = data['AccHard']\n",
    "data['Average $\\mathcal{W}$'] = data['AccSoft']\n",
    "data['InstructGran'] = data['Instruct'].apply(lambda x : 'Inst' if x else 'NoInst') + '_' + data['Granularity']\n",
    "order = data.groupby('Model')['AccHard'].mean().sort_values().index.tolist()\n",
    "order.remove('Average')\n",
    "order.append('Average')\n",
    "sns.set(rc={\"figure.figsize\":(12, 5)})\n",
    "sns.catplot(data=data, y='Model', x='Average $\\mathcal{R}$', hue='Instruct', order=order, kind='bar', col='Granularity', height=4, aspect=0.6);\n",
    "plt.savefig('plots/inst_vs_noninst_inst_vs_noninst_query_robust_all_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "\n",
    "order = data.groupby('Model')['AccSoft'].mean().sort_values().index.tolist()\n",
    "order.remove('Average')\n",
    "order.append('Average')\n",
    "sns.set(rc={\"figure.figsize\":(12, 5)})\n",
    "sns.catplot(data=data, y='Model', x='Average $\\mathcal{W}$', hue='Instruct', order=order, kind='bar', col='Granularity', height=4, aspect=0.6);\n",
    "plt.savefig('plots/inst_vs_noninst_inst_vs_noninst_query_winrate_all_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impossible to conclude something because instruct prompt are in the form of a question while the classical prompt is an affirmation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,5))\n",
    "sns.set(font_scale=1.1)\n",
    "\n",
    "data = data[data['Granularity'] == GLOBALGR]\n",
    "data['Input type'] = data['Instruct'].apply(lambda x: \"Instruction\" if x else \"Raw text\")\n",
    "data['Average $\\mathcal{R}_G$'] = data['Average $\\mathcal{R}$']\n",
    "sns.barplot(data=data, y='Model', x='Average $\\mathcal{R}_G$', hue='Input type', order=order);\n",
    "plt.ylabel('')\n",
    "plt.savefig('plots/inst_vs_noninst_inst_vs_noninst_query_robust_global_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model size and family vs. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "data = score_per_fact[~score_per_fact['Instruct']].reset_index(drop=True).merge(models_data[['Model', 'NumParams', 'Instruct']], on='Model', suffixes=['', 'Info'])\n",
    "data = data[data['Granularity'] == GLOBALGR]\n",
    "family = data['Model'].apply(get_model_family)\n",
    "# data['NumParams'] = np.log10(data['NumParams'])\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.lineplot(data,x='NumParams',y='AccHard', hue=family, style=data['InstructInfo'], markers='o', hue_order=sorted(family.unique()), errorbar=None)\n",
    "plt.xlabel('Number of parameters')\n",
    "plt.ylabel('Average $\\mathcal{R}_G$')\n",
    "plt.savefig('plots/numparams_vs_robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = score_per_fact[~score_per_fact['Instruct']].reset_index(drop=True).merge(models_data[['Model', 'NumParams', 'Instruct']], on='Model', suffixes=['', 'Info'])\n",
    "family = data['Model'].apply(get_model_family)\n",
    "data = data[(data['Granularity'] == GLOBALGR) & (family == 'Gemma-2') & (data['InstructInfo'])]\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.lineplot(data,x='NumParams',y='AccHard', style=data['InstructInfo'], markers='o', errorbar=None)\n",
    "plt.xlim((1,1000000000000000))\n",
    "plt.ylim((0,1))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of parameters')\n",
    "plt.ylabel('Average $\\mathcal{R}_G$')\n",
    "plt.savefig('plots/numparams_vs_robust_gemma.pdf', bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring temporal coherence\n",
    "\n",
    "The proportion of facts that are known in one granularity but not in the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "# all_combinations_gran = list(set(x) for x in powerset(['Day', 'Month', 'Year']))[1:]\n",
    "all_combinations_gran_str = [\n",
    "  'Year',\n",
    "  'Month',\n",
    "  'Day',\n",
    "  'Year+Month',\n",
    "  'Year+Day',\n",
    "  'Month+Day',\n",
    "  'Year+Month+Day',\n",
    "]\n",
    "all_combinations_gran = [set(x.split('+')) for x in all_combinations_gran_str]\n",
    "# all_combinations_gran_str = [all_combinations_gran_str[i] for i in order]\n",
    "# all_combinations_gran = [all_combinations_gran[i] for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(df : pd.DataFrame):\n",
    "    df = df[df['AccHard'] == 1]\n",
    "    gl = df['Granularity'].tolist()\n",
    "    \n",
    "    df = df.iloc[0]\n",
    "    df['KnownGran'] = '_'.join(sorted(gl))\n",
    "    df.drop(columns='AccHard', inplace=True)\n",
    "    return df\n",
    "\n",
    "data = score_per_fact[(score_per_fact['AccHard'] == 1) & ~score_per_fact['Instruct'] & (score_per_fact['Granularity'] != GLOBALGR)]\n",
    "known_gran = data.groupby(['Model','Fact'], sort=False)[['AccHard', 'Granularity']].apply(f).reset_index(drop=True)\n",
    "order = known_gran.groupby('KnownGran')['AccHard'].count().sort_values().index.tolist()\n",
    "known_gran['KnownGran'] = pd.Categorical(known_gran['KnownGran'], order)\n",
    "sns.histplot(data=known_gran, y=\"KnownGran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set(font_scale=1.5)\n",
    "for instruct in (True, False):\n",
    "    mask = ~score_per_fact['Instruct'] if not instruct else score_per_fact['Instruct']\n",
    "    for USE_ALL_COMBINATIONS in (False,):\n",
    "        if KEPT_MODELS is not None:\n",
    "            kept_models = KEPT_MODELS.copy()\n",
    "        else:\n",
    "            kept_models = score_per_fact[(score_per_fact['Granularity'] == GLOBALGR) & mask].groupby('Model')['AccHard'].mean().sort_values(ascending=False).index.tolist()[:5]\n",
    "        for model in kept_models:\n",
    "            print(model, instruct)\n",
    "            def f(df : pd.DataFrame):\n",
    "                l = set(df['Granularity'].to_list())\n",
    "                df2 = pd.DataFrame(data={\n",
    "                    'Fact': [df.name]*len(all_combinations_gran)\n",
    "                })\n",
    "                df2['CombGran'] = all_combinations_gran\n",
    "                success = [x.issubset(l) for x in all_combinations_gran]\n",
    "                df2['Success'] = success\n",
    "                return df2\n",
    "            data = score_per_fact[(score_per_fact['Model'] == model) & (score_per_fact['AccHard'] == 1) & mask & (score_per_fact['Granularity'] != GLOBALGR)]\n",
    "            known_gran = data.groupby(['Fact'], sort=False)[['Granularity']].apply(f).reset_index(drop=True)\n",
    "            if len(known_gran) == 0:\n",
    "                print('Skip %s' % model)\n",
    "                continue\n",
    "\n",
    "            def propci_wilson_cc(count, nobs, alpha=0.05):\n",
    "                # get confidence limits for proportion\n",
    "                # using wilson score method w/ cont correction\n",
    "                # i.e. Method 4 in Newcombe [1]; \n",
    "                # verified via Table 1\n",
    "                from scipy import stats\n",
    "                n = nobs\n",
    "                p = count/n\n",
    "                q = 1.-p\n",
    "                z = stats.norm.isf(alpha / 2.)\n",
    "                z2 = z**2   \n",
    "                denom = 2*(n+z2)\n",
    "                num = 2.*n*p+z2-1.-z*np.sqrt(z2-2-1./n+4*p*(n*q+1))    \n",
    "                ci_l = num/denom\n",
    "                num = 2.*n*p+z2+1.+z*np.sqrt(z2+2-1./n+4*p*(n*q-1))\n",
    "                ci_u = num/denom\n",
    "                if p == 0:\n",
    "                    ci_l = 0.\n",
    "                elif p == 1:\n",
    "                    ci_u = 1.\n",
    "                return (ci_l+ci_u)/2, (ci_u-ci_l)/2 \n",
    "\n",
    "            heatmap = np.zeros((len(all_combinations_gran), len(all_combinations_gran)))\n",
    "            cis = np.zeros_like(heatmap).astype(str)\n",
    "            for i in range(len(all_combinations_gran)):\n",
    "                for j in range(len(all_combinations_gran)):\n",
    "                    A = all_combinations_gran[i].union(all_combinations_gran[j])\n",
    "                    B = all_combinations_gran[j]\n",
    "                    A_B = A.intersection(B)\n",
    "                    B_occ = known_gran.loc[known_gran['CombGran'] == B, 'Success'].sum()\n",
    "                    A_occ = known_gran.loc[known_gran['CombGran'] == A, 'Success'].sum()\n",
    "                    A_B_occ = A_occ + B_occ - known_gran.loc[known_gran['CombGran'] == A_B, 'Success'].sum()\n",
    "                    cis[i,j] = '%.2f±\\n%.2f' % propci_wilson_cc(A_B_occ, B_occ) if not A.issubset(B) else 1\n",
    "                    heatmap[i,j] = A_B_occ / B_occ\n",
    "            plt.figure(figsize=(5,4))\n",
    "            if USE_ALL_COMBINATIONS:\n",
    "                sns.heatmap(heatmap, vmin=0, vmax=1, xticklabels=all_combinations_gran_str, yticklabels=all_combinations_gran_str, cmap=\"rocket\", fmt='', annot=cis, annot_kws={\"size\": 10})\n",
    "            else:\n",
    "                sns.heatmap(heatmap[:3,:3], vmin=0, vmax=1, xticklabels=all_combinations_gran_str[:3], yticklabels=all_combinations_gran_str[:3], cmap=\"rocket\", fmt='', annot=cis[:3,:3], annot_kws={\"size\": 20})\n",
    "            plt.ylabel('A')\n",
    "            plt.xlabel('B')\n",
    "            plt.title('P(A is known | B is known)')\n",
    "            plt.savefig('plots/gran_vs_%s_%s_%s_%s.pdf' % ('allcomb' if USE_ALL_COMBINATIONS else 'smallcomb', 'inst' if instruct else 'noninst', model, EXPERIMENT), bbox_inches='tight', pad_inches=0);\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global robustness\n",
    "for instruct in (True, False):\n",
    "    for USE_ALL_COMBINATIONS in (False,):\n",
    "        print(instruct)\n",
    "        def f(df : pd.DataFrame):\n",
    "            l = set(df['Granularity'].to_list())\n",
    "            df2 = pd.DataFrame(data={\n",
    "                'Fact': [df.name]*len(all_combinations_gran)\n",
    "            })\n",
    "            df2['CombGran'] = all_combinations_gran\n",
    "            success = [x.issubset(l) for x in all_combinations_gran]\n",
    "            df2['Success'] = success\n",
    "            return df2\n",
    "        mask = ~score_per_fact['Instruct'] if not instruct else score_per_fact['Instruct']\n",
    "        if KEPT_MODELS is not None:\n",
    "            kept_models = KEPT_MODELS.copy()\n",
    "        else:\n",
    "            kept_models = score_per_fact[(score_per_fact['Granularity'] == GLOBALGR) & mask].groupby('Model')['AccHard'].mean().sort_values(ascending=False).index.tolist()[:5]\n",
    "\n",
    "        data = score_per_fact[(score_per_fact['AccHard'] == 1) & mask & (score_per_fact['Granularity'] != GLOBALGR) & np.isin(score_per_fact['Model'], kept_models)]\n",
    "        known_gran = data.groupby(['Model','Fact'], sort=False)[['Granularity']].apply(f).reset_index(drop=True)\n",
    "\n",
    "        def propci_wilson_cc(count, nobs, alpha=0.05):\n",
    "            # get confidence limits for proportion\n",
    "            # using wilson score method w/ cont correction\n",
    "            # i.e. Method 4 in Newcombe [1]; \n",
    "            # verified via Table 1\n",
    "            from scipy import stats\n",
    "            n = nobs\n",
    "            p = count/n\n",
    "            q = 1.-p\n",
    "            z = stats.norm.isf(alpha / 2.)\n",
    "            z2 = z**2   \n",
    "            denom = 2*(n+z2)\n",
    "            num = 2.*n*p+z2-1.-z*np.sqrt(z2-2-1./n+4*p*(n*q+1))    \n",
    "            ci_l = num/denom\n",
    "            num = 2.*n*p+z2+1.+z*np.sqrt(z2+2-1./n+4*p*(n*q-1))\n",
    "            ci_u = num/denom\n",
    "            if p == 0:\n",
    "                ci_l = 0.\n",
    "            elif p == 1:\n",
    "                ci_u = 1.\n",
    "            return (ci_l+ci_u)/2, (ci_u-ci_l)/2 \n",
    "\n",
    "        heatmap = np.zeros((len(all_combinations_gran), len(all_combinations_gran)))\n",
    "        cis = np.zeros_like(heatmap).astype(str)\n",
    "        for i in range(len(all_combinations_gran)):\n",
    "            for j in range(len(all_combinations_gran)):\n",
    "                A = all_combinations_gran[i].union(all_combinations_gran[j])\n",
    "                B = all_combinations_gran[j]\n",
    "                A_B = A.intersection(B)\n",
    "                B_occ = known_gran.loc[known_gran['CombGran'] == B, 'Success'].sum()\n",
    "                A_occ = known_gran.loc[known_gran['CombGran'] == A, 'Success'].sum()\n",
    "                A_B_occ = A_occ + B_occ - known_gran.loc[known_gran['CombGran'] == A_B, 'Success'].sum()\n",
    "                cis[i,j] = '%.2f±\\n%.2f' % propci_wilson_cc(A_B_occ, B_occ) if not A.issubset(B) else 1\n",
    "                heatmap[i,j] = A_B_occ / B_occ\n",
    "        plt.figure(figsize=(5,4))\n",
    "        if USE_ALL_COMBINATIONS:\n",
    "            sns.heatmap(heatmap, vmin=0, vmax=1, xticklabels=all_combinations_gran_str, yticklabels=all_combinations_gran_str, cmap=\"rocket\", fmt='', annot=cis, annot_kws={\"size\": 10})\n",
    "        else:\n",
    "            sns.heatmap(heatmap[:3,:3], vmin=0, vmax=1, xticklabels=all_combinations_gran_str[:3], yticklabels=all_combinations_gran_str[:3], cmap=\"rocket\", fmt='', annot=cis[:3,:3], annot_kws={\"size\": 20})\n",
    "            avg_transfer = (heatmap[:3,:3].sum()-3)/6\n",
    "            print('Average=%s' % avg_transfer)\n",
    "        plt.ylabel('A')\n",
    "        plt.xlabel('B')\n",
    "        plt.title('P(A is known | B is known)')\n",
    "        plt.savefig('plots/gran_vs_%s_%s_alllms_%s.pdf' % ('allcomb' if USE_ALL_COMBINATIONS else 'smallcomb', 'inst' if instruct else 'noninst', EXPERIMENT), bbox_inches='tight', pad_inches=0);\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about date to date consistency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(df : pd.DataFrame):\n",
    "    correct_mask = df['IsCorrect'] == 'Correct'\n",
    "    incorrect_mask = df['IsCorrect'] == 'Incorrect'\n",
    "    matches = df.loc[correct_mask, 'CondLogProb'].values[:, None] > df.loc[incorrect_mask, 'CondLogProb'].values[None, :]\n",
    "    soft_score = matches.mean(0)\n",
    "    df['CorrectWinRateOverIncorrect'] = float('nan')\n",
    "    df['CorrectWinRateOverIncorrectInstruct'] = float('nan')\n",
    "    df.loc[incorrect_mask, 'CorrectWinRateOverIncorrect'] = soft_score\n",
    "    if not df['CondLogProbInstruct'].isna().all():\n",
    "        matches = df.loc[correct_mask, 'CondLogProbInstruct'].values[:, None] > df.loc[incorrect_mask, 'CondLogProbInstruct'].values[None, :]\n",
    "        soft_score = matches.mean(0)\n",
    "        df.loc[incorrect_mask, 'CorrectWinRateOverIncorrectInstruct'] = soft_score\n",
    "    df = df.sort_values('Alpha')\n",
    "    df['Position'] = np.arange(len(df))\n",
    "    df = df[df['IsCorrect'] == 'Incorrect']\n",
    "    return df[['Position', \"Alpha\", 'CorrectWinRateOverIncorrect', 'CorrectWinRateOverIncorrectInstruct']]\n",
    "logprobs_dedup2 = logprobs_dedup[~logprobs_dedup['Alpha'].isna() & np.isin(logprobs_dedup['Model'], kept_models)].groupby(['Fact', 'Model', 'Granularity'], sort=False)[['CondLogProbInstruct', 'CondLogProb', 'Alpha', 'IsCorrect']].apply(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Consistency between date precision   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_dedup2[['CorrectWinRateOverIncorrect2', 'CorrectWinRateOverIncorrectInstruct2']] = logprobs_dedup2[['CorrectWinRateOverIncorrect', 'CorrectWinRateOverIncorrectInstruct']] == 1\n",
    "data = logprobs_dedup2.groupby(['Model', 'Fact', 'Position'], sort=False)[['CorrectWinRateOverIncorrect2', 'CorrectWinRateOverIncorrectInstruct2']].sum()\n",
    "data = pd.concat([data, logprobs_dedup2.groupby(['Model', 'Fact', 'Position'], sort=False)['Alpha'].mean()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AlphaBin'] = pd.cut(data['Alpha'], bins=np.arange(-5,5.001,0.5))\n",
    "data2 = data[data['CorrectWinRateOverIncorrect2'] > 0]\n",
    "data2['CorrectWinRateOverIncorrect2'] = data2['CorrectWinRateOverIncorrect2'] == 3\n",
    "data2 = data2.reset_index()\n",
    "data2['FactStr'] = data2['Fact'].apply(str)\n",
    "weight = data2.value_counts([\"Model\", \"FactStr\"], normalize=True)\n",
    "weight.name = \"Weight\"\n",
    "data2 = data2.merge(weight, on=[\"Model\", \"FactStr\"])\n",
    "data2['Format'] = \"Raw text\"\n",
    "\n",
    "# Instruct\n",
    "data3 = data[data['CorrectWinRateOverIncorrectInstruct2'] > 0]\n",
    "data3['CorrectWinRateOverIncorrectInstruct2'] = data3['CorrectWinRateOverIncorrectInstruct2'] == 3\n",
    "data3 = data3.reset_index()\n",
    "data3['FactStr'] = data3['Fact'].apply(str)\n",
    "weight = data3.value_counts([\"Model\", \"FactStr\"], normalize=True)\n",
    "weight.name = \"Weight\"\n",
    "data3 = data3.merge(weight, on=[\"Model\", \"FactStr\"])\n",
    "data3['Format'] = \"Instruction\"\n",
    "data3['CorrectWinRateOverIncorrect2'] = data3['CorrectWinRateOverIncorrectInstruct2']\n",
    "\n",
    "data4 = pd.concat([data3,data2], ignore_index=True)\n",
    "\n",
    "sns.barplot(data4, x='AlphaBin', y='CorrectWinRateOverIncorrect2', hue='Format')\n",
    "plt.xlabel('$\\\\alpha$')\n",
    "plt.ylabel('Precision-wise consistency')\n",
    "plt.xticks(rotation=90)\n",
    "# plt.savefig('plots/granularity_consistency_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More sophisticated consistency through correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_dedup2['CorrectWinRateOverIncorrect2'] = logprobs_dedup2['CorrectWinRateOverIncorrect'] == 1\n",
    "logprobs_dedup2['CorrectWinRateOverIncorrectInstruct2'] = logprobs_dedup2['CorrectWinRateOverIncorrectInstruct'] == 1\n",
    "# data = logprobs_dedup2.groupby(['Model', 'Fact', 'Position'], sort=False)[['CorrectWinRateOverIncorrect2', 'CorrectWinRateOverIncorrectInstruct2']].sum()\n",
    "# data = pd.concat([data, logprobs_dedup2.groupby(['Model', 'Fact', 'Position'], sort=False)['Alpha'].mean()], axis=1)\n",
    "data = logprobs_dedup2.copy().reset_index()\n",
    "data['AlphaBin'] = pd.cut(data['Alpha'], bins=np.arange(-5,5.001,0.5))\n",
    "data = data[~data['AlphaBin'].isna()]\n",
    "data['FactStr'] = data.reset_index()['Fact'].apply(str).values  \n",
    "data = data.sort_values(['FactStr', 'Position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "def f(df : pd.DataFrame):\n",
    "    df = df.set_index(['Position', 'FactStr']) \n",
    "    ydf = df.loc[df['Granularity'] == TimeUnit.YEAR, ['CorrectWinRateOverIncorrect2', 'CorrectWinRateOverIncorrectInstruct2']]\n",
    "    ydf.rename(inplace=True, columns={\n",
    "        'CorrectWinRateOverIncorrect2' : 'CorrectWinRateOverIncorrect2Year',\n",
    "        'CorrectWinRateOverIncorrectInstruct2' : 'CorrectWinRateOverIncorrectInstruct2Year'\n",
    "    })\n",
    "    mdf = df.loc[df['Granularity'] == TimeUnit.MONTH, ['CorrectWinRateOverIncorrect2', 'CorrectWinRateOverIncorrectInstruct2']]\n",
    "    mdf.rename(inplace=True, columns={\n",
    "        'CorrectWinRateOverIncorrect2' : 'CorrectWinRateOverIncorrect2Month',\n",
    "        'CorrectWinRateOverIncorrectInstruct2' : 'CorrectWinRateOverIncorrectInstruct2Month'\n",
    "    })\n",
    "    ddf = df.loc[df['Granularity'] == TimeUnit.DAY, ['CorrectWinRateOverIncorrect2', 'CorrectWinRateOverIncorrectInstruct2']]\n",
    "    ddf.rename(inplace=True, columns={\n",
    "        'CorrectWinRateOverIncorrect2' : 'CorrectWinRateOverIncorrect2Day',\n",
    "        'CorrectWinRateOverIncorrectInstruct2' : 'CorrectWinRateOverIncorrectInstruct2Day'\n",
    "    })\n",
    "    df = pd.concat([mdf,ydf,ddf], axis=1)\n",
    "    df = df.loc[~df[['CorrectWinRateOverIncorrect2Month', 'CorrectWinRateOverIncorrectInstruct2Month', 'CorrectWinRateOverIncorrectInstruct2Day',\n",
    "                     'CorrectWinRateOverIncorrect2Year', 'CorrectWinRateOverIncorrectInstruct2Year', 'CorrectWinRateOverIncorrect2Day']].isna().any(axis=1)]\n",
    "    df[['CorrectWinRateOverIncorrect2Month', 'CorrectWinRateOverIncorrectInstruct2Month', 'CorrectWinRateOverIncorrectInstruct2Day', 'CorrectWinRateOverIncorrect2Day',\n",
    "        'CorrectWinRateOverIncorrect2Year', 'CorrectWinRateOverIncorrectInstruct2Year']] = \\\n",
    "        df[['CorrectWinRateOverIncorrect2Month', 'CorrectWinRateOverIncorrectInstruct2Month', \n",
    "        'CorrectWinRateOverIncorrectInstruct2Day', 'CorrectWinRateOverIncorrect2Day', 'CorrectWinRateOverIncorrect2Year', \n",
    "        'CorrectWinRateOverIncorrectInstruct2Year']].astype(int)\n",
    "    raw = 1/3*(matthews_corrcoef(df['CorrectWinRateOverIncorrect2Month'], df['CorrectWinRateOverIncorrect2Year']) \\\n",
    "        + matthews_corrcoef(df['CorrectWinRateOverIncorrect2Day'], df['CorrectWinRateOverIncorrect2Year'])\\\n",
    "        + matthews_corrcoef(df['CorrectWinRateOverIncorrect2Month'], df['CorrectWinRateOverIncorrect2Day']))\n",
    "    instruct = 1/3*(matthews_corrcoef(df['CorrectWinRateOverIncorrectInstruct2Month'], df['CorrectWinRateOverIncorrectInstruct2Year']) \\\n",
    "        + matthews_corrcoef(df['CorrectWinRateOverIncorrectInstruct2Day'], df['CorrectWinRateOverIncorrectInstruct2Year'])\\\n",
    "        + matthews_corrcoef(df['CorrectWinRateOverIncorrectInstruct2Month'], df['CorrectWinRateOverIncorrectInstruct2Day']))\n",
    "    return raw, instruct\n",
    "data2 = data.groupby(['Model', 'AlphaBin']).apply(f)\n",
    "data2.name = 'MCC'\n",
    "data2 = data2.reset_index()\n",
    "data2['Format'] = [('Raw text', 'Instruction')]*len(data2)\n",
    "data3 = data2.explode(['Format', 'MCC'])\n",
    "data3['AlphaBin2'] = data3['AlphaBin'].apply(lambda x : x.mid)\n",
    "plt.figure(figsize=(10,3))\n",
    "sns.barplot(data3, x='AlphaBin2', y='MCC', hue='Format', errorbar=None)\n",
    "plt.xlabel('$\\\\alpha$')\n",
    "plt.ylabel('Average correlation between\\ndate precisions')\n",
    "plt.xticks(range(len(data3['AlphaBin2'].cat.categories)),data3['AlphaBin'].cat.categories, rotation=90)\n",
    "plt.ylim((0.3,0.6))\n",
    "plt.savefig('plots/granularity_consistency_corr_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of win rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.histplot(data=score_per_fact.reset_index(), x='AccSoft', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best robustness model\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(data=score_per_fact.loc[score_per_fact['Model'] == 'google_gemma-2-27b-it', 'AccSoft'].reset_index(), x='AccSoft', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best win rate model\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(data=score_per_fact.loc[score_per_fact['Model'] == 'meta-llama_Llama-3.1-70B-Instruct', 'AccSoft'].reset_index(), x='AccSoft', bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where do language models fail with respect to the distance to the validity period?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where LMs fail when win rate is high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(alpha: float):\n",
    "    def t_(series : pd.Series):\n",
    "        mu = (np.abs(series) >= alpha).mean()\n",
    "        if alpha < 0.5:\n",
    "            p = 1\n",
    "        else:\n",
    "            p = (5-alpha) / 4.5\n",
    "        return mu\n",
    "    return t_\n",
    "to_plots = []\n",
    "alpha_range = np.arange(1,4.999,1)\n",
    "for instruct, mask in zip(('instruct', 'noninstruct'), (score_per_fact['Instruct'], ~score_per_fact['Instruct'])):\n",
    "    for th in (0.9,0.95,0.97,0.99):\n",
    "        print(instruct, th)\n",
    "        kept_models = score_per_fact[(score_per_fact['Granularity'] == GLOBALGR) & mask].groupby('Model')['AccHard'].mean().sort_values(ascending=False).index.tolist()[:5] \n",
    "        data = score_per_fact.loc[mask & (score_per_fact['Granularity'] == \"Year\") & np.isin(score_per_fact['Model'], kept_models),\n",
    "                                ['Model', 'AccSoft', 'WrongAlphaIncorrect', 'Fact']].copy()\n",
    "        data = data[(1 > data['AccSoft']) & (data['AccSoft'] >= th)]\n",
    "        print('len(data)', len(data))\n",
    "        data['FactStr'] = data['Fact'].apply(str)\n",
    "        weights = data.value_counts([\"Model\", \"FactStr\"], normalize=True)\n",
    "        weights.name = 'Weight'\n",
    "        data = data.explode('WrongAlphaIncorrect')\n",
    "        for alpha in alpha_range:\n",
    "            mu = data.groupby([\"Model\", 'FactStr'], sort=False)['WrongAlphaIncorrect'].apply(t(alpha)).to_frame()\n",
    "            mu['Alpha'] = int(alpha)\n",
    "            mu['Threshold'] = th\n",
    "            mu = mu.merge(weights, on=['Model', 'FactStr'])\n",
    "            mu['Instruct'] = instruct\n",
    "            to_plots.append(mu)\n",
    "to_plot = pd.concat(to_plots)\n",
    "values = []\n",
    "for instruct in ('instruct', 'noninstruct'):\n",
    "    to_plot2 = to_plot[to_plot['Instruct'] == instruct]\n",
    "    plt.figure(figsize=(5,4))\n",
    "    ax = sns.barplot(to_plot2, x='Alpha', y='WrongAlphaIncorrect', weights='Weight', hue='Threshold')\n",
    "    cis = np.stack([l.get_xydata()[:, 1] for l in ax.lines])\n",
    "    mid = cis.mean(1)\n",
    "    error = (cis[:,1] - cis[:,0]) / 2\n",
    "    values.append([\"%.3f±%.3f\" % (x,y) for x,y in zip(mid, error)])\n",
    "    plt.xlabel('$\\\\alpha$')\n",
    "    plt.xticks(list(range(len(alpha_range))), [\"$\\\\geq%s$\" % int(x) for x in alpha_range])\n",
    "    plt.ylabel('Proportion of incorrect dates\\nfavored over a correct date')\n",
    "    plt.savefig('plots/incorrect_date_position__th_%s_%s.pdf' % (instruct, EXPERIMENT), bbox_inches='tight', pad_inches=0);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha2succ = to_plot[to_plot['Threshold'] == 0.95].groupby(['Instruct', 'Alpha'])[['WrongAlphaIncorrect', 'Weight']].apply(lambda x : (x['WrongAlphaIncorrect'] * x['Weight']).sum())\n",
    "alpha2succ = alpha2succ.to_frame().reset_index()\n",
    "alpha2succ[\"Incorrect dates' proportion\"] = alpha2succ[0]\n",
    "alpha2succ = alpha2succ.drop(columns=0)\n",
    "alpha2succ['Alpha'] = alpha2succ['Alpha'].apply(lambda x : \"$\\\\geq %s$\" % int(x))\n",
    "alpha2succ = alpha2succ.set_index(['Alpha'])\n",
    "alpha2succ = pd.concat([alpha2succ[alpha2succ['Instruct'] != 'instruct'], alpha2succ[alpha2succ['Instruct'] == 'instruct']], axis=1).drop(columns='Instruct')\n",
    "alpha2succ.columns = ['Raw text', 'Instruction']\n",
    "values = np.array(values).reshape(2,4,4)\n",
    "alpha2succ['Raw text'] = values[1, 1, :]\n",
    "alpha2succ['Instruction'] = values[0, 1, :]\n",
    "print(alpha2succ.to_latex(index=True, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for instruct, mask in zip(('instruct', 'noninstruct'), (score_per_fact['Instruct'], ~score_per_fact['Instruct'])):\n",
    "#     print(instruct)\n",
    "#     kept_models = score_per_fact[(score_per_fact['Granularity'] == GLOBALGR) & mask].groupby('Model')['AccHard'].mean().sort_values(ascending=False).index.tolist()[:5] \n",
    "#     data = score_per_fact.loc[mask & (score_per_fact['Granularity'] != GLOBALGR) & np.isin(score_per_fact['Model'], kept_models),\n",
    "#                             ['Model', 'AccSoft', 'WrongAlphaIncorrect']].copy()\n",
    "#     data = data[(data['AccSoft'] > 0.95)]\n",
    "#     data = np.concatenate(data['WrongAlphaIncorrect'].tolist())\n",
    "#     plt.figure(figsize=(5,4))\n",
    "#     plt.xlabel('$\\\\alpha$')\n",
    "#     plt.ylabel('Proportion')\n",
    "#     weights = np.ones_like(data) / len(data)\n",
    "#     plt.hist(data, bins=30, weights=weights)\n",
    "#     plt.savefig('plots/incorrect_date_position_%s_%s.pdf' % (instruct, EXPERIMENT), bbox_inches='tight', pad_inches=0);\n",
    "#     plt.show()\n",
    "#     for alpha in (0,0.5,1,2,3,4,5):\n",
    "#         w = len(data[np.abs(data) >= alpha])  / len(data)\n",
    "#         print('Alpha >= %s --> %s' % (alpha, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are dynamic facts less likely to be memorized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidata_tools.core import TimedTriple, TripleQuery\n",
    "from wikidata_tools.wikidata import TempWikidata, WikidataPrepStage\n",
    "\n",
    "\n",
    "all_facts = score_per_fact['Fact'].unique()\n",
    "wd = TempWikidata(\"20210104\", WikidataPrepStage.PREPROCESSED)\n",
    "history_size = []\n",
    "for fact in all_facts:\n",
    "    fact : TimedTriple\n",
    "    history_size.append(len(list(wd.find(TripleQuery(fact.subject, fact.relation)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_history_size = pd.DataFrame({\n",
    "    'Fact' : all_facts,\n",
    "    'HistorySize' : np.log(1+np.array(history_size))\n",
    "})\n",
    "data = score_per_fact.merge(fact_history_size, on='Fact')\n",
    "data = data[data['Granularity'] != GLOBALGR]\n",
    "\n",
    "data_long = pd.melt(data, id_vars='HistorySize', value_vars=['AccSoft', 'AccHard'], \n",
    "                    var_name='Type', value_name='Performance')\n",
    "\n",
    "# Replace the variable names for better readability\n",
    "data_long.replace({'AccSoft': '$\\mathcal{W}$', 'AccHard': '$\\mathcal{R}$'}, inplace=True)\n",
    "\n",
    "# Plot the data\n",
    "sns.lmplot(data=data_long, x=\"HistorySize\", y='Performance', hue='Type', x_bins=list(range(0, 11)))\n",
    "plt.xlabel('Log History length of the fact (s,r,o)')\n",
    "\n",
    "plt.savefig('plots/perf_vs_historysize_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigger the history the larger chance of being memorized\n",
    "- More history means probably more times where the training dataset mentioned the fact and its frontier validity (positive corr.)\n",
    "- However, the longer the history the more contradictions in the trainig dataset  (negative corr.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are older facts more known when the granularity is year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = score_per_fact.copy()\n",
    "data = data[data['Granularity'] != GLOBALGR]\n",
    "data['Log distance in years between\\nstart date of the validity period and present'] = np.log(data['Fact'].apply(lambda x : 2021 - x.valid_between.start.year))\n",
    "sns.lmplot(data=data, x=\"Log distance in years between\\nstart date of the validity period and present\", y='AccSoft', hue='Granularity', x_bins=range(0, 10, 1))\n",
    "plt.ylabel('Average $\\mathcal{W}$')\n",
    "plt.savefig('plots/start_vs_lenient_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);\n",
    "sns.lmplot(data=data, x=\"Log distance in years between\\nstart date of the validity period and present\", y='AccHard', hue='Granularity', x_bins=range(0, 10, 1))\n",
    "plt.ylabel('Average $\\mathcal{R}$')\n",
    "plt.savefig('plots/start_vs_robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because data exist more on recent events than old ones, the model is biased towards recent knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is length period related to performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = score_per_fact.copy()\n",
    "data = data[data['Granularity'] != GLOBALGR]\n",
    "data['Log Duration of fact'] = np.log(data['Fact'].apply(lambda x : x.valid_between.end.year - x.valid_between.start.year))\n",
    "sns.lmplot(data=data, x=\"Log Duration of fact\", y='AccSoft', hue='Granularity', x_bins=range(0, 10), order=1)\n",
    "plt.ylabel('Average $\\mathcal{W}$')\n",
    "plt.savefig('duration_vs_lenient.pdf', bbox_inches='tight', pad_inches=0);\n",
    "sns.lmplot(data=data, x=\"Log Duration of fact\", y='AccHard', hue='Granularity', x_bins=range(0, 10), order=1)\n",
    "plt.ylabel('Average $\\mathcal{R}$')\n",
    "plt.savefig('plots/duration_vs_robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The higher the period length the less it is known because frequent changes on some fact implies more occurences of it in the dataset (positive correlation)\n",
    "- The higher the period length the less contradictions in the dataset (negative correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,6))\n",
    "data = score_per_fact.copy()\n",
    "data = data[data['Granularity'] == GLOBALGR]\n",
    "data['Relation'] = data['Fact'].apply(lambda x : x.relation.label)\n",
    "order = data.groupby('Relation')['AccHard'].mean().sort_values(ascending=False).index.tolist()[:10]\n",
    "sns.barplot(data=data, y='Relation', x='AccHard', order=order)\n",
    "plt.xlabel('Average $\\mathcal{R}_G$')\n",
    "plt.ylabel('')\n",
    "plt.savefig('plots/relation_vs_robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is popularity related to performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidata_tools.wikidata import WikidataPopularity\n",
    "\n",
    "\n",
    "data = score_per_fact.merge(logprobs_dedup[['Fact', 'FactPop']].drop_duplicates(), on='Fact')\n",
    "data = data[data['Granularity'] == GLOBALGR]\n",
    "\n",
    "wikipop = WikidataPopularity('20210104')\n",
    "data['ObjectPop'] = np.exp(wikipop.get_popularity(data['Fact'].apply(lambda x : x.object)))\n",
    "data['SubjectPop'] = np.exp(wikipop.get_popularity(data['Fact'].apply(lambda x : x.subject)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=data, y='AccHard', x='SubjectPop', x_bins=range(0, 12*10**5+1, 10**5), order=2)\n",
    "plt.ylabel('Average $\\mathcal{R}_G$')\n",
    "plt.xlabel('Popularity of $s$ in fact $(s,r,o)$')\n",
    "plt.ylim((-0.03,0.6))\n",
    "plt.savefig('plots/pop_sub_vs_robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=data, y='AccHard', x='ObjectPop', x_bins=range(0, 12*10**5+1, 10**5), order=2)\n",
    "plt.ylabel('Average $\\mathcal{R}_G$')\n",
    "plt.xlabel('Popularity of $o$ in fact $(s,r,o)$')\n",
    "plt.ylim((-0.03,0.6))\n",
    "plt.savefig('plots/pop_obj_vs_robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fact popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,10))\n",
    "sns.lmplot(data=data, y='AccHard', x='FactPop', x_bins=range(0, 12*10**5+1, 10**5), order=2)\n",
    "plt.ylabel('Average $\\mathcal{R}_G$')\n",
    "plt.xlabel('Popularity of fact')\n",
    "plt.ylim((-0.03,0.6))\n",
    "plt.savefig('plots/pop_fact_vs_robust_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha vs. Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Delta LogProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = logprobs_dedup.copy()\n",
    "data['Granularity'] = data['Granularity'].apply(lambda x : x.name.lower().capitalize())\n",
    "\n",
    "data['Date validity'] = data['IsCorrect']\n",
    "data = data[np.isin(data['IsCorrect'], ['Correct', 'Incorrect', 'Transitional'])]\n",
    "data['$\\\\alpha$'] = data['Alpha']\n",
    "data['$log\\ P(o \\\\mid f,d)$'] = data['CondLogProb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data, x='$\\\\alpha$', y='$log\\ P(o \\\\mid f,d)$', hue=\"Date validity\", x_bins=np.linspace(-5,5,100), col='Granularity', fit_reg=False, ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data[data['Granularity'] == 'Year'], x='$\\\\alpha$', y='$log\\ P(o \\\\mid f,d)$', hue=\"Date validity\", x_bins=np.linspace(-5,5,100), fit_reg=False, ci=None, \n",
    "           hue_order=['Correct', 'Incorrect', 'Transitional'], palette=['#007f4e', '#e12729', '#f37324'], markers=['*', 'x', 'o'],\n",
    "           height=5, aspect=1.2)\n",
    "plt.savefig('plots/alpha_vs_logprob_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "data_ = data.copy()\n",
    "data_['$\\\\alpha$'] = pd.cut(data_['Alpha'], bins=[-5,-3,-1,-0.5,-0.25,0,0.25,0.5,1,3,5]).astype('category')\n",
    "data_['$\\\\alpha$'] = data_['$\\\\alpha$'].cat.add_categories([-0.5,0.5,pd.Interval(-1.0, -0.5, 'neither'), pd.Interval(0.25, 0.5, 'neither')])\n",
    "# data_.loc[data_['IsCorrect'] == 'Transitional', 'Alpha'] = data_.loc[data_['IsCorrect'] == 'Transitional', 'Alpha'].apply(lambda value : min([-0.5,0.5], key=lambda x:abs(x-value)))\n",
    "data_.loc[data_['$\\\\alpha$'] == pd.Interval(-1.0, -0.5, 'right'), '$\\\\alpha$'] = pd.Interval(-1.0, -0.5, 'neither')\n",
    "data_.loc[data_['$\\\\alpha$'] == pd.Interval(0.25, 0.5, 'right'), '$\\\\alpha$'] = pd.Interval(0.25, 0.5, 'neither')\n",
    "data_.loc[data_['Alpha'] == 0.5, '$\\\\alpha$'] = 0.5\n",
    "data_.loc[data_['Alpha'] == -0.5, '$\\\\alpha$'] = -0.5\n",
    "data_['$\\\\alpha$'] = data_['$\\\\alpha$'].astype(str).astype('category')\n",
    "categories = [\n",
    "    \"(-5.0, -3.0]\",\n",
    "    \"(-3.0, -1.0]\",\n",
    "    \"(-1.0, -0.5)\",\n",
    "    \"-0.5\",\n",
    "    \"(-0.5, -0.25]\",\n",
    "    \"(-0.25, 0.0]\",\n",
    "    \"(0.0, 0.25]\",\n",
    "    \"(0.25, 0.5)\",\n",
    "    \"0.5\",\n",
    "    \"(0.5, 1.0]\",\n",
    "    \"(1.0, 3.0]\",\n",
    "    \"(3.0, 5.0]\"\n",
    "]\n",
    "reference = 10\n",
    "data_['$\\\\alpha$'] = data_['$\\\\alpha$'].cat.set_categories(categories, ordered=True)\n",
    "data_['$log\\ P(o \\\\mid f,d)$'] = data_['$log\\ P(o \\\\mid f,d)$'] + reference\n",
    "ax = sns.barplot(data_[data_['Granularity'] == 'Year'], x='$\\\\alpha$', y='$log\\ P(o \\\\mid f,d)$', hue=\"Date validity\", \n",
    "           hue_order=['Correct', 'Incorrect', 'Transitional'], palette=['#007f4e', '#e12729', '#f37324'],\n",
    "           dodge=False, \n",
    "           width=1)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "counts = data_[data_['Granularity'] == 'Year'].groupby('$\\\\alpha$').size()\n",
    "\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "ax.set_yticklabels([f'{label - reference}' for label in ax.get_yticks()])\n",
    "plt.ylim(0,3.5)\n",
    "plt.savefig('plots/alpha_vs_logprob_bar_%s.pdf' % EXPERIMENT, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model assigns a higher logprob of the good answer when it is close to the start of the validity period vs. the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data, x='Alpha', y='TimeWin', hue=\"IsCorrect\", x_bins=np.linspace(-5,5,100), col=\"Granularity\", fit_reg=False, ci=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
